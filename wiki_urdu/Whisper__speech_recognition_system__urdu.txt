وِسپَر ایک مشین لرننگ ماڈل ہے جو گفتگو کی شناخت اور نقل کے لیے استعمال ہوتا ہے، جسے OpenAI نے تیار کیا ہے اور پہلی بار ستمبر 2022 میں اوپن سورس سافٹ ویئر کے طور پر جاری کیا گیا۔  
یہ انگریزی اور کئی دیگر زبانوں میں تقریر کو نقل کرنے کے قابل ہے، اور ساتھ ہی متعدد غیر-English زبانوں کا انگریزی میں ترجمہ بھی کرنے کی صلاحیت رکھتا ہے۔ OpenAI کا دعویٰ ہے کہ اس کی تیاری میں استعمال شدہ مختلف تربیتی ڈیٹا کے امتزاج کی وجہ سے اس کے نتائج میں لہجوں، پس منظر کی آواز اور اصطلاحات کی پہچان میں بہتری آئی ہے، جو پچھلے طریقوں کے مقابلے میں زیادہ موثر ہے۔  
وِسپَر ایک کمزور نگرانی والے ڈیپ لرننگ اکوئسٹک ماڈل ہے، جسے انکوڈر-ڈیکوڈر ٹرانسفارمر ساخت میں تیار کیا گیا ہے۔  
وِسپَر لارج V2 8 دسمبر 2022 کو جاری کیا گیا۔ وِسپَر لارج V3 نومبر 2023 میں، OpenAI ڈیولپمنٹ ڈے کے موقع پر، متعارف کروائی گئی۔  

== پس منظر ==

گفتگو کی شناخت کا شعبہ تحقیق میں ایک تاریخی میدان رہا ہے؛ ابتدائی طریقوں میں شماریاتی طریقے استعمال کیے گئے، جیسے کہ متحرک وقت کی جنگ بندی ( dynamic time warping) اور بعد میں پوشیدہ مارکوف ماڈلز۔ تقریباً 2010 کے بعد، گہرے نیورل نیٹورک کے طریقے گفتگو کی شناخت کے لیے زیادہ استعمال ہونے لگے، جنہیں بڑے ڈیٹا سیٹس ("بگ ڈیٹا") اور بڑھتی ہوئی حسابی صلاحیت کے سبب ممکن بنایا گیا۔  
گفتگو کی شناخت میں گہرے لرننگ کے ابتدائی طریقوں میں کونولوشنل نیورل نیٹورکس شامل تھے، لیکن یہ محدود تھے کیونکہ یہ ترتیب وار ڈیٹا کو پکڑنے سے قاصر تھے، جس کے نتیجے میں Seq2seq طریقوں کی ترقی ہوئی، جن میں رکرنٹ نیورل نیٹورکس شامل ہیں جنہوں نے لمبی اور مختصر مدت یادداشت کا استعمال کیا۔  
20217 میں گوگل کی جانب سے متعارف کرائے گئے ٹرانسفارمرز نے مشین لرننگ کے بہت سے سابقہ سٹیٹ آف آرٹ طریقوں کو پیچھے چھوڑ دیا، اور زبان کے ماڈلنگ اور کمپیوٹر وژن جیسے شعبوں میں بنیادی نیورل ساخت کے طور پر اپنا کردار ادا کرنا شروع کیا؛  
2020 کے اوائل میں کمزور نگرانی والے اکوئسٹک ماڈلز کی تربیت کے لیے طریقے ان کے وعدہ مند ہونے کے طور پر تسلیم کیے گئے۔  
ایک NYT کی رپورٹ کے مطابق، 2021 میں OpenAI نے سمجھا کہ انہوں نے اپنے بڑے زبان ماڈلز کی تربیت کے لیے اعلیٰ معیار کے ڈیٹا کے وسائل ختم کر لیے ہیں، اور انہوں نے ویب سے حاصل کیے گئے متن کو یوٹیوب ویڈیوز اور پوڈکاسٹس کے ٹرانسکرپشنز کے ساتھ مکمل کرنے کا فیصلہ کیا، اور اسی مقصد کے لیے وِسپَر تیار کیا۔  

== ساخت ==

وِسپَر کی ساخت ایک انکوڈر-ڈیکوڈر ٹرانسفارمر پر مبنی ہے۔  
**انپٹ آڈیو** کو 16,000 ہرٹز پر ری سیمپل کیا جاتا ہے اور 25 ملی سیکنڈ کی ونڈوز کے ساتھ 10 ملی سیکنڈ کے اسٹرا ئڈ پر 80 چینلز کے لوگ-مگنیٹیوڈ میل اسپیکٹروگرام میں تبدیل کیا جاتا ہے۔  
اس اسپیکٹروگرام کو پھر [\(-1, 1\)] رینج میں نارملائز کیا جاتا ہے، جس کا اوسط تقریباً صفر ہوتا ہے۔  
انکوڈر اس میل اسپیکٹروگرام کو انپٹ کے طور پر لیتا ہے اور اس پر عملدرآمد کرتا ہے۔ یہ پہلے دو کونولوشنل لیئرز سے گزرتا ہے۔  
سائنسولڈل پوزیشنل ایمبیڈنگز شامل کی جاتی ہیں۔ اس کے بعد اسے ٹرانسفارمر انکوڈر بلاکس کے سلسلے کے ذریعے پروسیس کیا جاتا ہے (پری-ایکٹیویشن ریزڈیول کنکشنز کے ساتھ)۔  
انکوڈر کا آؤٹ پٹ لئیر نارملائزڈ ہوتا ہے۔  

ڈیکوڈر ایک معمول کا ٹرانسفارمر ڈیکوڈر ہے۔  
اس میں انکوڈر جتنی ہی چوڑائی اور ٹرانسفارمر بلاکس شامل ہیں۔  
یہ سیکھے گئے پوزیشنل ایمبیڈنگز اور باندھے ہوئے انپٹ-آؤٹ پٹ ٹوکن خصوصیات استعمال کرتا ہے (دونوں کے لیے ایک ہی وہیٹ میٹرکس استعمال کرتے ہوئے)۔  
یہ ایک بائٹ-پئر انکوڈنگ ٹوکنائزر استعمال کرتا ہے، جیسا کہ GPT-2 میں ہوتا ہے۔  
صرف انگریزی کے ماڈلز GPT-2 کا لغت استعمال کرتے ہیں، جبکہ متعدد زبانوں کے ماڈلز ایک دوبارہ تربیت شدہ ملٹی لینگوئل لغت استعمال کرتے ہیں، جس میں الفاظ کی تعداد تقریباً ایک جیسی ہوتی ہے۔  

خاص ٹوکنز کا استعمال کیا جاتا ہے تاکہ ڈیکوڈر متعدد کام انجام دے سکے:  
- زبان ظاہر کرنے والے ٹوکنز (ہر زبان کے لیے ایک منفرد ٹوکن)۔  
- کام بتانے والے ٹوکنز (<|transcribe|> یا <|translate|>)۔  
- اگر ٹائم اسٹامپ موجود نہ ہوں تو ظاہر کرنے والے ٹوکن (<|notimestamps|>)۔ اگر یہ ٹوکن موجود نہ ہو، تو ڈیکوڈر حصے کے حوالے سے ٹائم اسٹامپ کی پیشن گوئی کرتا ہے اور اسے 20 ملی سیکنڈ کے وقفوں میں کوانٹائز کرتا ہے۔  
- <|nospeech|> آواز کی سرگرمی کی تشخیص کے لیے۔  
- <|startoftranscript|> اور <|endoftranscript|>۔  
جو بھی متن <|startoftranscript|> سے پہلے آتا ہے، وہ ڈیکوڈر سے نہیں نکلتا بلکہ سیاق کے طور پر فراہم کیا جاتا ہے۔  
نقصان صرف غیر سیاقی حصوں پر حساب کیا جاتا ہے، یعنی ان خاص ٹوکنز کے درمیان کے ٹوکنز پر۔

ٹریننگ ڈیٹاسیٹ میں انٹرنیٹ سے حاصل شدہ 680,000 گھنٹے کے لیبل شدہ آڈیو-ٹرانسکرپٹ جوڑوں پر مشتمل ہے۔ اس میں 96 غیر انگلش زبانوں میں 117,000 گھنٹے شامل ہیں اور 125,000 گھنٹے X→انگریزی ترجمہ ڈیٹا، جہاں X کسی بھی غیر انگلش زبان کی نمائندگی کرتا ہے، شامل ہیں۔  
پری پراسیسنگ میں ٹرانسکرپٹس کے معیاری بنانا، خسروچی استعمال کرکے مشین سے تیار شدہ ٹرانسکرپٹس کو فلٹر کرنا (مثلاً، نقطہ، ہجے، عنوانات)، زبان کی شناخت اور ان کے ساتھ ٹرانسکرپٹس کا میل، فزی ڈی ڈپلیکیشن، اور ڈیٹا آلودگی سے بچاؤ کے لیے تشخیصی ڈیٹا سیٹس کے ساتھ ڈیپلیکیشن شامل ہے۔  
بغیر آواز کے حصے بھی شامل کیے گئے، تاکہ وائس ایکٹیویٹی ڈیٹیکشن کی تربیت کی جا سکے۔  
ان فائلوں کے لیے جو فلٹرنگ کے بعد بھی باقی رہ گئی تھیں، آڈیو فائلز کو 30 سیکنڈ کے حصوں میں توڑ دیا گیا اور ان کے ساتھ اُس ٹرانسکرپٹ کا سب سیٹ جو اُس وقت کے اندر آتا ہے۔  
اگر یہ متوقع بولی جانے والی زبان اس متن کے ساتھ منسلک زبان سے مختلف ہوتی تھی، تو وہ آڈیو-ٹرانسکرپٹ جوڑی ٹریننگ کے لیے استعمال نہیں کی گئی، بلکہ ترجمہ کی تربیت کے لیے استعمال کی گئی۔  

=== تربیت کے بعد فلٹرنگ ===

پہلی ماڈل کی تربیت کے بعد، اس کو مختلف ذیلی سیٹ پر چلایا گیا، جو تربیت کے مختلف ماخذ کی نمائندگی کرتے تھے۔  
ماخذ کی درجہ بندی ان کے غلطی کی شرح اور حجم کے مجموعہ سے کی گئی۔  
سب سے اعلیٰ درجہ بندی والے ذرائع کی دستی جانچ (زیادہ غلطی، بڑا حجم) سے یہ تعین کیا گیا کہ آیا ماخذ کم معیار کا ہے (مثلاً، جزوی ٹرانسکرپشنز، غلط مطابقت)۔  
کم معیار کے ماخذ کو بعد میں ہٹا دیا گیا۔  

== تربیت ==

وِسپر کو نیم نگرانی والی تعلیمی طریقہ کار سے 680,000 گھنٹوں کا ملٹی لسانی اور ملٹی ٹاسک ڈیٹا استعمال کرکے تربیت دیا گیا، جن میں سے تقریباً ایک پانچواں حصہ (117,000 گھنٹے) غیر انگلش آڈیو ڈیٹا تھا۔  
تربیت کے بعد، اس کو اسپیکر کے نام کی پیشن گوئی کو روکنے کے لیے بہتر بنایا گیا۔  
اس کی تربیت AdamW آپٹیمائزر سے کی گئی جس میں گریڈینٹ نارم کلپنگ اور وارن اپ کے ساتھ لائنر لئیرننگ ریٹ ڈی کل تھی، اور بیچ سائز 256 حصے رکھا گیا۔  
تربیت 1 ملین اپ ڈیٹس (2-3 تہذیبیں) کے لیے جاری رہی۔  
کسی بھی قسم کا ڈیٹا اُستوار کرنا یا ریگولرائزیشن نہیں کی گئی، سوائے Large V2 ماڈل کے، جس میں SpecAugment، stochastic depth، اور BPE dropout کا استعمال کیا گیا۔  
تربیت میں ڈیٹا پیراللزم، فلوٹ 16، متحرک نقصان اسکیلنگ، اور ایکٹیویشن چیکپوائنٹنگ استعمال کیے گئے۔  

== صلاحیت ==

وِسپر ان ماڈلز سے بہتر کارکردگی کا مظاہرہ نہیں کرتا جو لیبری اسپیچ ڈیٹا سیٹ میں مہارت رکھتے ہیں، لیکن جب اسے بہت سے ڈیٹا سیٹس پر ٹیسٹ کیا گیا، تو یہ زیادہ مستحکم ثابت ہوا اور دیگر ماڈلز سے 50% کم غلطیاں کیں۔  
وِسپر کا غلطی کا تناسب مختلف زبانوں میں مختلف ہے، اور ان زبانوں میں زیادہ ہے جو تربیتی ڈیٹا میں اچھے انداز میں شامل نہیں ہیں۔  
مصنفین نے پایا کہ ملٹی ٹاسک سیکھنے سے مجموعی کارکردگی میں بہتری آئی، بہ نسبت ایسے ماڈلز کے جو صرف ایک کام پر مہارت رکھتے ہیں۔  
انہوں نے قیاس کیا کہ بہترین وِسپر ماڈل ابھی بھی ڈیٹا سیٹ کے حساب سے کم فٹنگ کرتا ہے، اور بڑے ماڈلز اور طویل تربیت سے بہتر نتائج حاصل کیے جا سکتے ہیں۔  
تیسری پارٹی کی جانچوں میں مختلف سطحوں پر AI ہالوسی نیشن دیکھی گئی ہے۔  
ایک مطالعہ میں عوامی ملاقاتوں کے ٹرانسکرپٹس میں ہر 10 میں سے 8 میں ہالوسی نیشنز پائی گئیں، جبکہ ایک انجینئر نے 100 گھنٹوں کی ٹرانسکرپشنز میں سے "تقريباً نصف" میں ہالوسی نیشن دیکھی، اور ایک ڈویلپر نے 26,000 ٹرانسکرپٹس میں سے "تقريباً ہر ایک" میں یہ دریافت کیا۔  
ایک مطالعہ جس میں 13,140 مختصر آڈیو حصے (معمولاً 10 سیکنڈ) شامل تھے، میں 187 ہالوسی نیشنز (1.4%) سامنے آئیں، جن میں سے 38% متن میں حروف یا چیزوں کے حوالے سے غلط ریفرنسز شامل کرنے، جھوٹی ادویات، یا پرتشدد واقعات جو آڈیو میں نہیں تھے، پیدا کیے گئے۔  

== استعمالات ==

یہ ماڈل کئی ایپلی کیشنز کے لیے بنیادی طور پر استعمال ہوا ہے، جیسے کہ تقریر کی شناخت کا واحد ماڈل اور عمومی آواز کی شناخت میں بھی۔